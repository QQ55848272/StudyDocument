# **ğŸ“Œ ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰å­¦ä¹ æŒ‡å—**

ç›‘ç£å­¦ä¹ æ˜¯**æœºå™¨å­¦ä¹ ï¼ˆMachine Learningï¼‰\**ä¸­çš„æ ¸å¿ƒé¢†åŸŸä¹‹ä¸€ï¼Œå¹¿æ³›åº”ç”¨äº\**å›¾åƒè¯†åˆ«ã€è¯­éŸ³å¤„ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€é‡‘èåˆ†æ**ç­‰é¢†åŸŸã€‚æœ¬æ–‡å°†ä»‹ç»**æ ¸å¿ƒæ¦‚å¿µã€ç®—æ³•ã€å­¦ä¹ èµ„æºã€ä»£ç å®è·µ**ï¼Œé€‚åˆä¸åŒé˜¶æ®µçš„å­¦ä¹ è€…ã€‚

------

## **ğŸ“š 1. ç›‘ç£å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ**

### **âœ… ç›‘ç£å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ**

ç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œè®­ç»ƒæ•°æ®é›†åŒ…å«**è¾“å…¥æ•°æ®ï¼ˆXï¼‰å’Œå¯¹åº”çš„æ­£ç¡®è¾“å‡ºï¼ˆYï¼‰**ï¼Œç®—æ³•å­¦ä¹ ä»**è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºçš„å‡½æ•°**ï¼Œå¹¶ç”¨äºé¢„æµ‹æ–°æ•°æ®çš„ç»“æœã€‚

ğŸ“Œ **å…¬å¼**ï¼š
y=f(x)+Ïµy = f(x) + \epsilony=f(x)+Ïµ
å…¶ä¸­ï¼š

- xxxï¼šè¾“å…¥ç‰¹å¾
- yyyï¼šç›®æ ‡å€¼ï¼ˆæ ‡ç­¾ï¼‰
- f(x)f(x)f(x)ï¼šå­¦ä¹ åˆ°çš„æ˜ å°„å‡½æ•°
- Ïµ\epsilonÏµï¼šå™ªå£°ï¼ˆè¯¯å·®é¡¹ï¼‰

------

## **ğŸ“Š 2. ä¸»è¦çš„ç›‘ç£å­¦ä¹ ç®—æ³•**

### **1ï¸âƒ£ çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰**

- ä»»åŠ¡ï¼šç”¨äº**é¢„æµ‹æ•°å€¼**ï¼ˆå¦‚æˆ¿ä»·é¢„æµ‹ï¼‰

- å…³é”®å…¬å¼ï¼š
  y=w0+w1x1+w2x2+â‹¯+wnxny = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_ny=w0â€‹+w1â€‹x1â€‹+w2â€‹x2â€‹+â‹¯+wnâ€‹xnâ€‹

- ä¼˜åŒ–æ–¹æ³•

  ï¼š

  - **æœ€å°äºŒä¹˜æ³•ï¼ˆLeast Squaresï¼‰**
  - **æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰**

ğŸ“Œ **Python ä»£ç ç¤ºä¾‹**ï¼š

```
pythonå¤åˆ¶ç¼–è¾‘from sklearn.linear_model import LinearRegression
import numpy as np

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 4, 6, 8, 10])

model = LinearRegression()
model.fit(X, y)

print("é¢„æµ‹:", model.predict([[6]]))  # é¢„æµ‹ y(6)
```

------

### **2ï¸âƒ£ é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰**

- ä»»åŠ¡ï¼šç”¨äº**äºŒåˆ†ç±»é—®é¢˜**ï¼ˆå¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹ï¼‰
- å…¬å¼ï¼ˆSigmoid å‡½æ•°ï¼‰ï¼š P(y=1âˆ£x)=11+eâˆ’(w0+w1x1+w2x2+â‹¯+wnxn)P(y=1 | x) = \frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n)}}P(y=1âˆ£x)=1+eâˆ’(w0+w1x1+w2x2+â‹¯+wnxn)1

ğŸ“Œ **Python ä»£ç ç¤ºä¾‹**ï¼š

```
pythonå¤åˆ¶ç¼–è¾‘from sklearn.linear_model import LogisticRegression

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([0, 0, 1, 1, 1])  # äºŒåˆ†ç±»æ ‡ç­¾

model = LogisticRegression()
model.fit(X, y)

print("é¢„æµ‹:", model.predict([[3.5]]))  # é¢„æµ‹åˆ†ç±»
```

------

### **3ï¸âƒ£ å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰**

- ä»»åŠ¡ï¼šç”¨äºåˆ†ç±»å’Œå›å½’ï¼ˆå¦‚ä¿¡ç”¨è¯„åˆ†ï¼‰
- å…³é”®æ¦‚å¿µï¼š
  - **ä¿¡æ¯å¢ç›Šï¼ˆInformation Gainï¼‰**
  - **Gini ä¸çº¯åº¦ï¼ˆGini Impurityï¼‰**

ğŸ“Œ **Python ä»£ç ç¤ºä¾‹**ï¼š

```
pythonå¤åˆ¶ç¼–è¾‘from sklearn.tree import DecisionTreeClassifier

X = np.array([[1], [2], [3], [4], [5]])
y = np.array([0, 0, 1, 1, 1])

model = DecisionTreeClassifier()
model.fit(X, y)

print("é¢„æµ‹:", model.predict([[3.5]]))
```

------

### **4ï¸âƒ£ éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰**

- ä»»åŠ¡ï¼šå¤šä¸ªå†³ç­–æ ‘ç»„æˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›ï¼ˆå¦‚äººè„¸è¯†åˆ«ï¼‰
- ä¼˜åŠ¿ï¼š**å‡å°‘è¿‡æ‹Ÿåˆï¼Œæå‡å‡†ç¡®ç‡**

ğŸ“Œ **Python ä»£ç ç¤ºä¾‹**ï¼š

```
pythonå¤åˆ¶ç¼–è¾‘from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=10)
model.fit(X, y)

print("é¢„æµ‹:", model.predict([[3.5]]))
```

------

### **5ï¸âƒ£ æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰**

- ä»»åŠ¡ï¼šç”¨äº**åˆ†ç±»ä»»åŠ¡**ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ï¼‰
- å…³é”®æ¦‚å¿µï¼š
  - **è¶…å¹³é¢ï¼ˆHyperplaneï¼‰**
  - **æ ¸å‡½æ•°ï¼ˆKernel Functionï¼‰**

ğŸ“Œ **Python ä»£ç ç¤ºä¾‹**ï¼š

```
pythonå¤åˆ¶ç¼–è¾‘from sklearn.svm import SVC

model = SVC(kernel='linear')
model.fit(X, y)

print("é¢„æµ‹:", model.predict([[3.5]]))
```

------

### **6ï¸âƒ£ ç¥ç»ç½‘ç»œï¼ˆNeural Networksï¼‰**

- ä»»åŠ¡ï¼šç”¨äºå¤æ‚ä»»åŠ¡ï¼ˆå¦‚è¯­éŸ³è¯†åˆ«ã€æ·±åº¦å­¦ä¹ ï¼‰
- å…³é”®æ¦‚å¿µï¼š
  - **å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰**
  - **åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰**
  - **æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ï¼ˆGradient Descentï¼‰**

ğŸ“Œ **Python ä»£ç ç¤ºä¾‹ï¼ˆMLP ç¥ç»ç½‘ç»œï¼‰**ï¼š

```
pythonå¤åˆ¶ç¼–è¾‘from sklearn.neural_network import MLPClassifier

model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)
model.fit(X, y)

print("é¢„æµ‹:", model.predict([[3.5]]))
```

------

## **ğŸ“– 3. ç»å…¸ä¹¦ç±æ¨è**

### **1. ã€Šæœºå™¨å­¦ä¹ ã€‹ - å‘¨å¿—å**

- **ä¼˜ç‚¹**ï¼šå›½å†…æƒå¨æ•™æï¼Œæ¶µç›–ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€‚
- **é€‚åˆäººç¾¤**ï¼šæƒ³ç³»ç»Ÿå­¦ä¹ æœºå™¨å­¦ä¹ æ•°å­¦åŸç†çš„å­¦ç”Ÿã€‚

### **2. ã€ŠPattern Recognition and Machine Learningã€‹ - Christopher Bishop**

- **ä¼˜ç‚¹**ï¼šç»å…¸æ•™æï¼Œè¯¦ç»†æ¨å¯¼è´å¶æ–¯æ–¹æ³•ï¼Œé€‚åˆæ•°å­¦åŸºç¡€å¥½çš„äººã€‚

### **3. ã€ŠHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlowã€‹ - AurÃ©lien GÃ©ron**

- **ä¼˜ç‚¹**ï¼šç»“åˆä»£ç å’Œç†è®ºï¼Œé€‚åˆ Python æ–¹å‘çš„æœºå™¨å­¦ä¹ å®è·µè€…ã€‚

------

## **ğŸ¥ 4. åœ¨çº¿è¯¾ç¨‹æ¨è**

### **1. Coursera - æœºå™¨å­¦ä¹ ï¼ˆAndrew Ngï¼‰**

- **ä¼˜ç‚¹**ï¼šæ–¯å¦ç¦ç»å…¸è¯¾ç¨‹ï¼Œé€‚åˆåˆå­¦è€…ã€‚
- **é“¾æ¥**ï¼š[Coursera - Machine Learning](https://www.coursera.org/learn/machine-learning)

### **2. Fast.ai - Practical Deep Learning**

- **ä¼˜ç‚¹**ï¼šé€‚åˆæƒ³å¿«é€Ÿä¸Šæ‰‹æ·±åº¦å­¦ä¹ çš„äººã€‚
- **é“¾æ¥**ï¼š[Fast.ai - æ·±åº¦å­¦ä¹ è¯¾ç¨‹](https://course.fast.ai/)

### **3. MIT 6.036 - æœºå™¨å­¦ä¹ **

- **ä¼˜ç‚¹**ï¼šMIT è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹ï¼Œæ¶µç›–ç›‘ç£å­¦ä¹ ã€ç¥ç»ç½‘ç»œã€‚
- **é“¾æ¥**ï¼šMIT 6.036 - Machine Learning

------

## **ğŸ”¬ 5. æ¨èå­¦ä¹ è·¯å¾„**

1ï¸âƒ£ **åˆå­¦è€…**

- çœ‹ **Andrew Ng è¯¾ç¨‹**ï¼ŒæŒæ¡ç›‘ç£å­¦ä¹ åŸºç¡€ã€‚
- è¯» **ã€Šæœºå™¨å­¦ä¹ ã€‹å‘¨å¿—å**ï¼Œç†è§£æ•°å­¦åŸç†ã€‚

2ï¸âƒ£ **ä¸­çº§è¿›é˜¶**

- è¯» **ã€ŠHands-On Machine Learningã€‹**ï¼Œç»“åˆä»£ç å®æˆ˜ã€‚
- ç”¨ **sklearn / TensorFlow** ç»ƒä¹ æ¨¡å‹è®­ç»ƒã€‚

3ï¸âƒ£ **é«˜çº§ç ”ç©¶**

- å­¦ä¹  **æ·±åº¦å­¦ä¹ **ï¼ˆå¦‚ CNNã€Transformerï¼‰ã€‚
- ç ”ç©¶ **è´å¶æ–¯æ–¹æ³•ã€å¼ºåŒ–å­¦ä¹ ã€å› æœæ¨ç†**ã€‚